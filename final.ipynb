{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLycAYallNoEMGmXN9Vz+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zeba-Kauser/Web-Scraping_Project/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Web Scraping Project***"
      ],
      "metadata": {
        "id": "J_4RcOGkzgn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project  I am using  http://books.toscrape.com/index.html which is a website specifically designed for web scraping purposes.\n"
      ],
      "metadata": {
        "id": "fjYym_WevgzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Task: Grab the Category, Name, Rating, Price, and Image URL information for all 1000 products, and store the data in a CSV file for presentation in Power BI."
      ],
      "metadata": {
        "id": "jDdnfc-AxGvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the the necessary  libraries we need to scrape a website.**"
      ],
      "metadata": {
        "id": "slCFIPh6_r0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests  # Used for making HTTP requests\n",
        "from bs4 import BeautifulSoup  # to scrape information from web page\n",
        "from urllib.parse import urljoin  # to constructs a full (absolute) URL by combining a base URL with another URL.\n",
        "import csv  # for working with CSV files"
      ],
      "metadata": {
        "id": "C-zUytYO_zOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Let's get the Header of the website"
      ],
      "metadata": {
        "id": "iA2wTuAR6DZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://books.toscrape.com/index.html'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')  # Specify 'html.parser' as the parser\n",
        "Header = soup.header.text.strip()  # Find the header within the <header> tag\n",
        "print(Header)"
      ],
      "metadata": {
        "id": "xa7gRKZxz0Yt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's figure out the URL structure to go through  all pages category wise"
      ],
      "metadata": {
        "id": "ZS3TOQuxzpwO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyUMl0dN7Uwk",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#the URL to scrape\n",
        "url = 'https://books.toscrape.com/index.html'\n",
        "\n",
        "response = requests.get(url) # Make an HTTP request to get the HTML content of the page\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser') # Create a BeautifulSoup object to parse the HTML content\n",
        "\n",
        "books = soup.select('li > ul > li > a') # Select 'a' elements within the nested list structure using CSS selector\n",
        "\n",
        "# Initialize an empty list to store category URLs\n",
        "category_links = []\n",
        "\n",
        "# Loop through each 'a' element and extract the 'href' attribute\n",
        "for book in books:\n",
        "    link = book.get('href')  # Get the 'href' attribute from the 'a' tag\n",
        "    category_url = urljoin(url, link)  # Create absolute URLs by joining base URL with relative URLs\n",
        "    category_links.append(category_url)  # Append the absolute URL to the list\n",
        "\n",
        "category_links"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some categorys have additional pages.\n",
        "Let's checks each category page for the presence of extra pages."
      ],
      "metadata": {
        "id": "4DXwG_ZlDWXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = category_links\n",
        "additonal_pages = []\n",
        "# Loop through each category link\n",
        "for category_link in category_links:\n",
        "  response = requests.get(category_link)\n",
        "  soup = BeautifulSoup(response.content,'html')\n",
        "  category = soup.find('h1').text.strip() #Find the category title within an 'h1' tag and extract its text\n",
        "  Extra_pages = soup.find('form').text.strip() #Find a form element and extract its text\n",
        "  print(category)\n",
        "  print(Extra_pages)"
      ],
      "metadata": {
        "id": "-A3eUM8zDlKY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see categories with additional pages showing text 'showing 1 to 20'\n",
        "So  we will Check each category page information with  presence of the text 'to' and extracts the total number of pages.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wLIUiB8ZGvp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the total number of pages from the text content"
      ],
      "metadata": {
        "id": "W7g4FGhHzSRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = category_links\n",
        "# Loop through each category link\n",
        "for category_link in category_links:\n",
        "  response = requests.get(category_link)\n",
        "  soup = BeautifulSoup(response.content,'html')\n",
        "  pages = soup.find('form').text\n",
        "  # To Check if the text 'to' is present in the pages information\n",
        "  if 'to' in pages:\n",
        "    nav_bar = soup.find('ul', class_='pager')\n",
        "    all_pages = nav_bar.text.strip()\n",
        "    page_range = int(all_pages[10])\n",
        "    print(page_range)"
      ],
      "metadata": {
        "id": "9cTuGlpWzQXn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We  allready have the url of  index page. Now  we need  url starting from 2 page.   \n",
        "We can see that the URL structure of aditonal page is  following:\n",
        "\n",
        "https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html\n",
        "\n",
        "Index page URL structure is the following formate:\n",
        "\n",
        "https://books.toscrape.com/catalogue/category/books/mystery_3/index.html\n",
        "\n",
        "we can create additional URLs by replacing the **'index.html'** part with **'page-{page_num}.html'** where page_num ranges from 2 to total number of pages."
      ],
      "metadata": {
        "id": "Uc0rDXn6o1Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "additional_pages = []\n",
        "for category_link in category_links:\n",
        "  # Iterate through page numbers from 2 to page_range\n",
        "  for page_num in range(2,page_range+1):\n",
        "      page_url = category_link.replace('index.html', f'page-{page_num}.html') # Generate a new page URL by replacing 'index.html' with 'page-{page_num}.html'\n",
        "      additional_pages.append(page_url)\n",
        "additional_pages"
      ],
      "metadata": {
        "id": "baXt8wsh68wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The additional_pages list  contains URLs for additional pages for each category\n",
        "Now let's add all the URLs from additional_pages to category_links\n"
      ],
      "metadata": {
        "id": "9pXWtPNDDUBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend the category_links list with the additional_pages list\n",
        "category_links.extend(additional_pages)\n",
        "# Print the modified link_list\n",
        "category_links"
      ],
      "metadata": {
        "id": "q6LCM7SU9ZZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets collect information about books from categeory URLs.\n",
        "The category, title, rating, price, and the absolute URL of the book image."
      ],
      "metadata": {
        "id": "ufljrO3nKfth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []  # Initialize an empty list to store extracted data\n",
        "url_list = category_links  # Assign the list of category links to the variable url_list\n",
        "\n",
        "# Iterate through each URL in the category_links list\n",
        "for url in url_list:\n",
        "    # Send an HTTP GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Create a BeautifulSoup object to parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "    # Use the title tag to get the page title\n",
        "    title_tags = soup.find('h1')\n",
        "    category = title_tags.text  # Extract the text content of the title_tags\n",
        "\n",
        "    # Find all book articles on the page\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # Iterate through each book on the page\n",
        "    for book in books:\n",
        "        # Extract book information\n",
        "        title = book.h3.a['title']  # Extract the 'title' attribute from the 'a' tag inside the 'h3' tag\n",
        "\n",
        "        # Find the star rating of the book\n",
        "        rating_tag = book.find_next('p', class_='star-rating')\n",
        "        rating = rating_tag['class'][1] if rating_tag and 'class' in rating_tag.attrs else 'N/A'\n",
        "\n",
        "        # Find the price of the book\n",
        "        price_tag = book.find_next('p', class_='price_color')\n",
        "        price = price_tag.get_text() if price_tag else 'N/A'\n",
        "\n",
        "        # Find the absolute URL of the book image\n",
        "        img_tag = book.find_next('img')\n",
        "        absolute_image_url = urljoin(url, img_tag.get('src')) if img_tag and 'src' in img_tag.attrs else 'N/A'\n",
        "\n",
        "        # Append book data to the data list\n",
        "        data.append([category, title, rating, price, absolute_image_url])\n"
      ],
      "metadata": {
        "id": "77k4rUnd-Did"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the extracted data for each book."
      ],
      "metadata": {
        "id": "9p9fk6paOb-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cat_data in data:\n",
        "    print(cat_data)"
      ],
      "metadata": {
        "id": "Z2Avhw5KJInD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the information in csv file"
      ],
      "metadata": {
        "id": "qqcdRe7sPLEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns and data\n",
        "columns = ['category','BookTitle','rating','price','image_url']\n",
        "\n",
        "# Specify the file name\n",
        "file_name = 'All_product.csv'\n",
        "\n",
        "# Writing to the CSV file\n",
        "with open(file_name, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header (columns) to the CSV file\n",
        "    writer.writerow(columns)\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    writer.writerows(data)\n",
        "\n",
        "# Print a success message indicating that the CSV file has been created\n",
        "print(f'The CSV file \"{file_name}\" has been created successfully.')"
      ],
      "metadata": {
        "id": "WMFwYH3TtQJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download the csv file All_product and load it into Power BI to create reports."
      ],
      "metadata": {
        "id": "o2owjfh7QsQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mU5y8AfrRqYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}